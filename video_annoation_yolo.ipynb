{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Annotation-of-vehciles-in-image-by-YOLO3-Python\" data-toc-modified-id=\"Annotation-of-vehciles-in-image-by-YOLO3-Python-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Annotation of vehciles in image by YOLO3-Python</a></span></li><li><span><a href=\"#Data-preparation\" data-toc-modified-id=\"Data-preparation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Video-resizing-code\" data-toc-modified-id=\"Video-resizing-code-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Video resizing code</a></span></li></ul></li><li><span><a href=\"#setting-up-the-Virtual-Machine,-gcp\" data-toc-modified-id=\"setting-up-the-Virtual-Machine,-gcp-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>setting up the Virtual Machine, gcp</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setting-up-the-yolo-python-library\" data-toc-modified-id=\"Setting-up-the-yolo-python-library-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Setting up the yolo python <a href=\"https://github.com/madhawav/YOLO3-4-Py\" rel=\"nofollow\" target=\"_blank\">library</a></a></span></li><li><span><a href=\"#compiling-yolo-py-in-nvidia-docker\" data-toc-modified-id=\"compiling-yolo-py-in-nvidia-docker-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>compiling yolo-py in nvidia-docker</a></span></li><li><span><a href=\"#Downloading-and-compiling-darknet\" data-toc-modified-id=\"Downloading-and-compiling-darknet-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Downloading and compiling darknet</a></span></li><li><span><a href=\"#Download-and-compile-YOLO3-4-Py\" data-toc-modified-id=\"Download-and-compile-YOLO3-4-Py-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Download and compile YOLO3-4-Py</a></span></li><li><span><a href=\"#Run-test\" data-toc-modified-id=\"Run-test-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Run test</a></span></li><li><span><a href=\"#running-the-teensorflow-GPU-instance\" data-toc-modified-id=\"running-the-teensorflow-GPU-instance-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>running the teensorflow GPU instance</a></span></li><li><span><a href=\"#trail-attempt-1\" data-toc-modified-id=\"trail-attempt-1-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>trail attempt 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#ensure-nividia-docker-uses-the-GPU,-using-a-benchmark-described-here\" data-toc-modified-id=\"ensure-nividia-docker-uses-the-GPU,-using-a-benchmark-described-here-3.7.1\"><span class=\"toc-item-num\">3.7.1&nbsp;&nbsp;</span>ensure nividia-docker uses the GPU, using a benchmark described <a href=\"https://marmelab.com/blog/2018/03/21/using-nvidia-gpu-within-docker-container.html\" rel=\"nofollow\" target=\"_blank\">here</a></a></span></li></ul></li><li><span><a href=\"#trail-attempt-2\" data-toc-modified-id=\"trail-attempt-2-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>trail attempt 2</a></span><ul class=\"toc-item\"><li><span><a href=\"#creating-videos-from-the-csv-files\" data-toc-modified-id=\"creating-videos-from-the-csv-files-3.8.1\"><span class=\"toc-item-num\">3.8.1&nbsp;&nbsp;</span>creating videos from the csv files</a></span></li></ul></li><li><span><a href=\"#create-images\" data-toc-modified-id=\"create-images-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>create images</a></span></li><li><span><a href=\"#Create-a-video-with-high-resolution-images\" data-toc-modified-id=\"Create-a-video-with-high-resolution-images-3.10\"><span class=\"toc-item-num\">3.10&nbsp;&nbsp;</span>Create a video with high resolution images</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "<div id='id-section1'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video resizing code\n",
    "1. used moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import ntpath\n",
    "#from moviepy.editor import \n",
    "import moviepy.editor as mp\n",
    "\n",
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)\n",
    "\n",
    "movfiles=glob.glob('/home/*.mp4')\n",
    "\n",
    "imgpath='/home/imgs/'\n",
    "\n",
    "for idx, movs in enumerate(movfiles):\n",
    "    filename=path_leaf(movs).split('.')[0]\n",
    "    newpath = imgpath+filename+'_resized' \n",
    "    clip = mp.VideoFileClip(movs)\n",
    "    clip_resized = clip.resize(height=360)\n",
    "    clip_resized.write_videofile(newpath+'.mp4')\n",
    "    print 'done with '+str(idx)+'/'+str(len(movfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import ntpath\n",
    "from moviepy.editor import *\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)\n",
    "\n",
    "#movfiles=glob.glob('/media/sunbird/UDISK/video/*.mp4')\n",
    "\n",
    "imgpath='/home/images'\n",
    "\n",
    "def extract_frames(movie, times, imgdir):\n",
    "    clip = VideoFileClip(movie)\n",
    "    for t in times:\n",
    "        imgpath = os.path.join(imgdir, 'frame_{}.png'.format(t))\n",
    "        clip.save_frame(imgpath, t)\n",
    "\n",
    "movfiles=['/home/20180813_110045A.mp4']\n",
    "\n",
    "for idx, movs in enumerate(movfiles):\n",
    "    print path_leaf(movs).split('.')[0]\n",
    "    newpath = imgpath+path_leaf(movs).split('.')[0] \n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    clip = VideoFileClip(movs)\n",
    "    frames = clip.duration\n",
    "    frameneed=np.arange(0,frames,1)\n",
    "    extract_frames(movs,frameneed, newpath)\n",
    "    print 'done with '+str(idx)+'/'+str(len(movfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create 100 each sub folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import glob, os, shutil\n",
    "\n",
    "def grouper(S, n):\n",
    "    iterator = iter(S)\n",
    "    while True:\n",
    "        items = list(itertools.islice(iterator, n))\n",
    "        if len(items) == 0:\n",
    "            break\n",
    "        yield items\n",
    "        \n",
    "\n",
    "fnames = sorted(glob.glob('*.png'))\n",
    "for i, fnames in enumerate(grouper(fnames, 100)):\n",
    "    dirname = 'batch%d' % i\n",
    "    os.mkdir(dirname)\n",
    "    for fname in fnames:\n",
    "        shutil.move(fname, dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting up the Virtual Machine, gcp\n",
    "<div id='id-section2'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tranfering the images using gcloud scp\n",
    "```\n",
    "gcloud compute scp /home/sunbird/Documents/UEmissInfo/lab/emission-industry/fieldwork/odk/patna/dashcam/aug13/morning/images20180813_112806A.tar.gz instance-4:~/home/airpollutionstudyindia\n",
    "```    \n",
    "1. the above was not working , so used google drive to upload using browser\n",
    "1. setup a seprate hardisk to easy moving of files\n",
    "```\n",
    "sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/sdb\n",
    "sudo mount -o discard,defaults /dev/sdb /home/ubuntu/dashbord\n",
    "https://drive.google.com/open?id=\n",
    "https://drive.google.com/open?id=\n",
    "https://drive.google.com/open?id=1HLkGTMYUmov6otTJwBXJ7TvGaBVvQFCi\n",
    "``` \n",
    "1. to download from google drive used this\n",
    "```\n",
    "wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=107N9oaulkO5WsTCar-mV9v9zb3k7t3Jc' -O images20180813_112806A.tar.gz\n",
    "wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1dyREMf-2iNXOqTsJuEc6Z_vWHbolFuM5' -O images20180813_110045A.tar.gz\n",
    "wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1HLkGTMYUmov6otTJwBXJ7TvGaBVvQFCi' -O images20180813_114233A.tar.gz\n",
    "```\n",
    "1. to create a new dpeelearniing vm provided by gcp, based on [this](https://cloud.google.com/deep-learning-vm/docs/cli)\n",
    "```\n",
    "export IMAGE_FAMILY=\"tf-latest-cu92\"\n",
    "export ZONE=\"us-central1-c\t\"\n",
    "export INSTANCE_NAME=\"dl-tf-01\"\n",
    "gcloud compute instances create $INSTANCE_NAME \\\n",
    "  --zone=$ZONE \\\n",
    "  --image-family=$IMAGE_FAMILY \\\n",
    "  --image-project=deeplearning-platform-release \\\n",
    "  --maintenance-policy=TERMINATE \\\n",
    "  --accelerator='type=nvidia-tesla-k80,count=1' \\\n",
    "  --metadata='install-nvidia-driver=True'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the yolo python [library](https://github.com/madhawav/YOLO3-4-Py)\n",
    "\n",
    "1. The Yolo library is c++ code, for a lrge number of images and to get the bounding box of detected classes in the image with list labels, this API works fine\n",
    "1. A docker image was made along with GPU enabled\n",
    "1. The docker was installed with edit commited [here](https://github.com/nishadhka/YOLO3-4-Py/blob/master/docker/Dockerfile)\n",
    "```\n",
    "sudo docker build -f /home/sunbird/yolo-python/Dockerfile . -t airpollutionstudyindia/yolo-python:version1\n",
    "```\n",
    "1. the ran the docker by\n",
    "```\n",
    "sudo docker run -dit airpollutionstudyindia/yolo-python:version1\n",
    "```\n",
    "1. then got into its bash by\n",
    "```\n",
    "sudo docker exec -it 77a7b80573ba bash\n",
    "```\n",
    "1. made GPU enabled yolo-python by, after entering into YOLO3-4-Py, which is in /\n",
    "```\n",
    "export GPU=1\n",
    "export OPENCV=1\n",
    "pip3 install .\n",
    "```\n",
    "1. the above command ends without any error, try to test the docker_demo.py, but ends in error of ```killed``` which indicates the GPU compilation works :)\n",
    "1. made the commit of the image by\n",
    "```\n",
    "sudo docker commit -a \"nishadhka\" -m \"Docker image compiled for GPU enabled YOLO3-4-Py\" 77a7b80573ba airpollutionstudyindia/yolo-python:version2\n",
    "```\n",
    "1. add the container to GCP's container registry, which can be made using gcloud which is default in any GCP instances, however the instance has to be started with setting up with scope to read and write like [this](--scopes https://www.googleapis.com/auth/devstorage.read_write), on the other hand docker hub gives unlimited public repostoy, so better to go with docker hub\n",
    "```\n",
    "sudo docker login\n",
    "sudo docker push airpollutionstudyindia/yolo-python:version2\n",
    "```\n",
    "1. within the GPU tensorflow images used following code to run the above build\n",
    "```\n",
    "docker run --runtime=nvidia --rm airpollutionstudyindia/yolo-python:version2 nvidia-smi\n",
    "```\n",
    "1. this ends in error porbably there is no nvidia in that image\n",
    "1. running the container\n",
    "```\n",
    "docker run -dit airpollutionstudyindia/yolo-python:version2\n",
    "docker exec -it 07494f0a17ca1dd6c6 bash\n",
    "```\n",
    "1. copy file from it\n",
    "```\n",
    "sudo docker cp 07494f0a17ca:/YOLO3-4-Py/output/batch_0.csv /home/sunbird/\n",
    "gcloud compute scp gpu-tensorflow-2-vm:/home/sunbird/batch_0.csv /home/sunbird/Documents/UEmissInfo/lab/emission-industry/fieldwork/odk/patna/dashcam/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## running the code in it\n",
    "\n",
    "wget --no-check-certificate 'https://docs.google.com/uc?id=0B3X9GlR6EmbnQ0FtZmJJUXEyRTA&export=download' -O gdrive-linux-x64\n",
    "chmod a+x gdrive-linux-x64\n",
    "\n",
    "./gdrive-linux-x64 download 107N9oaulkO5WsTCar-mV9v9zb3k7t3Jc\n",
    "\n",
    "./gdrive-linux-x64 download 1dyREMf-2iNXOqTsJuEc6Z_vWHbolFuM5\n",
    "\n",
    "./gdrive-linux-x64 download 1HLkGTMYUmov6otTJwBXJ7TvGaBVvQFCi\n",
    "\n",
    "tar xf images20180813_112806A.tar.gz\n",
    "\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "pngfiles=glob.glob('*/*.png')\n",
    "\n",
    "newpnglist = []\n",
    "for i in range(0,len(pngfiles),50):\n",
    "     newpnglist.append(pngfiles[i], pngfiles[i+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydarknet import Detector, Image\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "net = Detector(bytes(\"cfg/yolov3.cfg\", encoding=\"utf-8\"), bytes(\"weights/yolov3.weights\", encoding=\"utf-8\"), 0, bytes(\"cfg/coco.data\",encoding=\"utf-8\"))\n",
    "\n",
    "input_files = os.listdir(\"input\")\n",
    "input_files.pop(0)\n",
    "werf=[]\n",
    "for file_name in input_files:\n",
    "    img = cv2.imread(os.path.join(\"input\",file_name))\n",
    "    img2 = Image(img)\n",
    "    results = net.detect(img2)\n",
    "    dbgh1=[]\n",
    "    for cat, score, bounds in results:\n",
    "        resdict={}\n",
    "        resdict['cat']=cat\n",
    "        resdict['score']=score\n",
    "        resdict['bounds']=bounds\n",
    "        resdict['filename']=file_name\n",
    "        dbgh=pd.DataFrame.from_dict(resdict)\n",
    "        dbgh1.append(dbgh)\n",
    "    dbgh2=pd.concat(dbgh1)    \n",
    "    werf.append(dbgh2)\n",
    "    \n",
    "werf1=pd.concat(werf)\n",
    "werf1.to_csv('test_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydarknet import Detector, Image\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "\n",
    "pngfiles=glob.glob('fsimgspa/images20180813_112806A/*/*.png')\n",
    "\n",
    "pngbatches=zip(*[iter(pngfiles)]*50)\n",
    "\n",
    "net = Detector(bytes(\"cfg/yolov3.cfg\", encoding=\"utf-8\"), bytes(\"weights/yolov3.weights\", encoding=\"utf-8\"), 0, bytes(\"cfg/coco.data\",encoding=\"utf-8\"))\n",
    "\n",
    "\n",
    "for idsb, batch in enumerate(pngbatches):\n",
    "    werf=[]\n",
    "    for ids, file_name in enumerate(batch):\n",
    "        img = cv2.imread(file_name)\n",
    "        img2 = Image(img)\n",
    "        results = net.detect(img2)\n",
    "        dbgh1=[]\n",
    "        print('complete',str(ids),'/',len(batch))\n",
    "        for cat, score, bounds in results:\n",
    "            resdict={}\n",
    "            resdict['cat']=cat\n",
    "            resdict['score']=score\n",
    "            resdict['bounds']=bounds\n",
    "            resdict['filename']=file_name\n",
    "            dbgh=pd.DataFrame.from_dict(resdict)\n",
    "            dbgh1.append(dbgh)\n",
    "        dbgh2=pd.concat(dbgh1)    \n",
    "        werf.append(dbgh2)\n",
    "    werf1=pd.concat(werf)\n",
    "    werf1.to_csv('output/batch_'+str(idsb)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. the above script took 17:04-19:35 for 6 batches without GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo docker build -f /home/sunbird/yolo-python/Dockerfile . -t airpollutionstudyindia/yolo-python:version1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compiling yolo-py in nvidia-docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. running the container\n",
    "```\n",
    "docker run -dit nvidia/cuda:latest\n",
    "docker exec -it e328b70ac89f bash\n",
    "```\n",
    "1. to find the distribution of nvicidia/cuda\n",
    "```\n",
    "cat /etc/*-release\n",
    "```\n",
    "1. it is found to be ubuntu 16.04, so stright forward to install the yolo-python\n",
    "```\n",
    "apt-get install -y python3.5\n",
    "apt-get install -y python3-pip\n",
    "apt-get install -y git\n",
    "apt-get install -y unzip wget screen\n",
    "\n",
    "\n",
    "apt-get install -y build-essential cmake\n",
    "apt-get install -y qt5-default libvtk6-dev\n",
    "apt-get install -y zlib1g-dev libjpeg-dev libwebp-dev libpng-dev libtiff5-dev libjasper-dev libopenexr-dev libgdal-dev\n",
    "apt-get install -y libdc1394-22-dev libavcodec-dev libavformat-dev libswscale-dev libtheora-dev libvorbis-dev libxvidcore-dev libx264-dev yasm libopencore-amrnb-dev libopencore-amrwb-dev libv4l-dev libxine2-dev\n",
    "apt-get install -y python-dev python-tk python-numpy python3-dev python3-tk python3-numpy\n",
    "\n",
    "\n",
    "\n",
    "## Downloading and compiling darknet ##\n",
    "apt-get install -y git\n",
    "git clone https://github.com/pjreddie/darknet.git\n",
    "cd darknet\n",
    "make\n",
    "export DARKNET_HOME=/home/darknet\n",
    "export LD_LIBRARY_PATH=/home/darknet\n",
    "\n",
    "## Download and compile YOLO3-4-Py ##\n",
    "git clone https://github.com/madhawav/YOLO3-4-Py.git\n",
    "pip3 install pkgconfig\n",
    "pip3 install cython\n",
    "#pip3 install yolo34py-gpu\n",
    "\n",
    "export GPU=1\n",
    "export OPENCV=0\n",
    "python3 setup.py build_ext --inplace\n",
    "\n",
    "\n",
    "## Run test ##\n",
    "RUN sh download_models.sh\n",
    "ADD ./docker_demo.py /YOLO3-4-Py/docker_demo.py\n",
    "CMD [\"python3\", \"docker_demo.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running the teensorflow GPU instance\n",
    "1. It is found that instead of a docker, it is better to go with pre created GPU enabled instance avaiable in GCP\n",
    "1. One such image is Deep Learning VM, which is low cost 0.4 us$ per hour while comparing it with Nivida based images which use high end GPU\n",
    "1. The Deep learning VM was run on a 2 core, 14 GB ram machine with K80 GPU\n",
    "1. The following setup was made to isntall the yolo2--4-py, based on [this](https://github.com/madhawav/YOLO3-4-Py/blob/master/docker/Dockerfile), this is based on ubuntu16.04, unfortunalty the GCP VM image is debian 9\n",
    "```\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y build-essential cmake\n",
    "sudo apt-get install -y qt5-default libvtk6-dev\n",
    "```\n",
    "1. there is issue in installing jasper library, based on [this](https://discuss.96boards.org/t/opencv-3-2-install-dependencies-error/2139/2), that is not working and based on [this](https://www.raspberrypi.org/forums/viewtopic.php?t=203006) found a not a essential lib\n",
    "```\n",
    "sudo apt-get install -y zlib1g-dev libjpeg-dev libwebp-dev libpng-dev libtiff5-dev libopenexr-dev libgdal-dev\n",
    "sudo apt-get install -y libdc1394-22-dev libavcodec-dev libavformat-dev libswscale-dev libtheora-dev libvorbis-dev libxvidcore-dev \n",
    "sudo apt-get install -y unzip wget\n",
    "wget https://github.com/opencv/opencv/archive/3.4.0.zip\n",
    "unzip 3.4.0.zip\n",
    "cd opencv-3.4.0\n",
    "mkdir build\n",
    "cd /opencv-3.4.0/build\n",
    "cmake -DBUILD_EXAMPLES=OFF ..\n",
    "make\n",
    "make install\n",
    "ldconfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trail attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM ubuntu:16.04\n",
    "RUN apt-get update\n",
    "LABEL maintainer \"NVIDIA CORPORATION <cudatools@nvidia.com>\"\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends ca-certificates apt-transport-https gnupg-curl && \\\n",
    "    rm -rf /var/lib/apt/lists/* && \\\n",
    "    NVIDIA_GPGKEY_SUM=d1be581509378368edeec8c1eb2958702feedf3bc3d17011adbf24efacce4ab5 && \\\n",
    "    NVIDIA_GPGKEY_FPR=ae09fe4bbd223a84b2ccfce3f60f4b3d7fa2af80 && \\\n",
    "    apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub && \\\n",
    "    apt-key adv --export --no-emit-version -a $NVIDIA_GPGKEY_FPR | tail -n +5 > cudasign.pub && \\\n",
    "    echo \"$NVIDIA_GPGKEY_SUM  cudasign.pub\" | sha256sum -c --strict - && rm cudasign.pub && \\\n",
    "    echo \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 /\" > /etc/apt/sources.list.d/cuda.list && \\\n",
    "    echo \"deb https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64 /\" > /etc/apt/sources.list.d/nvidia-ml.list\n",
    "\n",
    "ENV CUDA_VERSION 9.2.148\n",
    "\n",
    "ENV CUDA_PKG_VERSION 9-2=$CUDA_VERSION-1\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "        cuda-cudart-$CUDA_PKG_VERSION && \\\n",
    "    ln -s cuda-9.2 /usr/local/cuda && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# nvidia-docker 1.0\n",
    "LABEL com.nvidia.volumes.needed=\"nvidia_driver\"\n",
    "LABEL com.nvidia.cuda.version=\"${CUDA_VERSION}\"\n",
    "\n",
    "RUN echo \"/usr/local/nvidia/lib\" >> /etc/ld.so.conf.d/nvidia.conf && \\\n",
    "    echo \"/usr/local/nvidia/lib64\" >> /etc/ld.so.conf.d/nvidia.conf\n",
    "\n",
    "ENV PATH /usr/local/nvidia/bin:/usr/local/cuda/bin:${PATH}\n",
    "ENV LD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
    "\n",
    "# nvidia-container-runtime\n",
    "ENV NVIDIA_VISIBLE_DEVICES all\n",
    "ENV NVIDIA_DRIVER_CAPABILITIES compute,utility\n",
    "ENV NVIDIA_REQUIRE_CUDA \"cuda>=9.2\"\n",
    "\n",
    "RUN apt-get update\n",
    "\n",
    "## Pyton installation ##\n",
    "RUN apt-get install -y python3.5\n",
    "RUN apt-get install -y python3-pip\n",
    "RUN apt-get install -y git\n",
    "\n",
    "## OpenCV 3.4 Installation ##\n",
    "RUN apt-get install -y build-essential cmake\n",
    "RUN apt-get install -y qt5-default libvtk6-dev\n",
    "RUN apt-get install -y zlib1g-dev libjpeg-dev libwebp-dev libpng-dev libtiff5-dev libjasper-dev libopenexr-dev libgdal-dev\n",
    "RUN apt-get install -y libdc1394-22-dev libavcodec-dev libavformat-dev libswscale-dev libtheora-dev libvorbis-dev libxvidcore-dev libx264-dev yasm libopencore-amrnb-dev libopencore-amrwb-dev libv4l-dev libxine2-dev\n",
    "RUN apt-get install -y python-dev python-tk python-numpy python3-dev python3-tk python3-numpy\n",
    "RUN apt-get install -y unzip wget\n",
    "RUN wget https://github.com/opencv/opencv/archive/3.4.0.zip\n",
    "RUN unzip 3.4.0.zip\n",
    "RUN rm 3.4.0.zip\n",
    "WORKDIR /opencv-3.4.0\n",
    "RUN mkdir build\n",
    "WORKDIR /opencv-3.4.0/build\n",
    "RUN cmake -DBUILD_EXAMPLES=OFF ..\n",
    "RUN make -j4\n",
    "RUN make install\n",
    "RUN ldconfig\n",
    "\n",
    "## Downloading and compiling darknet ##\n",
    "WORKDIR /\n",
    "RUN apt-get install -y git\n",
    "RUN git clone https://github.com/pjreddie/darknet.git\n",
    "WORKDIR /darknet\n",
    "# Set OpenCV makefile flag\n",
    "RUN sed -i '/OPENCV=0/c\\OPENCV=1' Makefile\n",
    "RUN make\n",
    "ENV DARKNET_HOME /darknet\n",
    "ENV LD_LIBRARY_PATH /darknet\n",
    "\n",
    "## Download and compile YOLO3-4-Py ##\n",
    "WORKDIR /\n",
    "RUN git clone https://github.com/madhawav/YOLO3-4-Py.git\n",
    "WORKDIR /YOLO3-4-Py\n",
    "RUN pip3 install pkgconfig\n",
    "RUN pip3 install cython\n",
    "ENV GPU=1\n",
    "ENV OPENCV=1\n",
    "RUN python3 setup.py build_ext --inplace\n",
    "RUN pip3 install pandas\n",
    "RN apt-get install screen\n",
    "RUN sh download_models.sh \n",
    "\n",
    "## Run test ##\n",
    "#RUN sh download_models.sh\n",
    "#ADD ./docker_demo.py /YOLO3-4-Py/docker_demo.py\n",
    "#CMD [\"python3\", \"docker_demo.py\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docker build -f dockerfile . -t airpollutionstudyindia/nvidia-yolo-py:version1\n",
    "docker login\n",
    "docker push airpollutionstudyindia/nvidia-yolo-py:version1\n",
    "1. running the container\n",
    "```\n",
    "docker run --runtime=nvidia --rm airpollutionstudyindia/nvidia-yolo-py:version1 nvidia-smi\n",
    "docker run -dit airpollutionstudyindia/nvidia-yolo-py:version1\n",
    "docker exec -it 52ccbbf7678b17 bash\n",
    "```\n",
    "1. to setup the gdrive download and download the imagery by \n",
    "```\n",
    "wget --no-check-certificate 'https://docs.google.com/uc?id=0B3X9GlR6EmbnQ0FtZmJJUXEyRTA&export=download' -O gdrive-linux-x64\n",
    "chmod a+x gdrive-linux-x64\n",
    "./gdrive-linux-x64 download 107N9oaulkO5WsTCar-mV9v9zb3k7t3Jc\n",
    "./gdrive-linux-x64 download 1dyREMf-2iNXOqTsJuEc6Z_vWHbolFuM5\n",
    "./gdrive-linux-x64 download 1HLkGTMYUmov6otTJwBXJ7TvGaBVvQFCi\n",
    "tar xf images20180813_112806A.tar.gz\n",
    "tar xf images20180813_110045A.tar.gz\n",
    "tar xf images20180813_114233A.tar.gz\n",
    "```\n",
    "1. get the model by running the script in folder YOLO3-4-py\n",
    "```\n",
    "sh download_models.sh\n",
    "```\n",
    "1. commit the image and it has to be run with nividia run time\n",
    "```\n",
    "docker commit -a \"nishadhka\" -m \"YOLO3-4-py with models\" 52ccbbf7678b airpollutionstudyindia/nvidia-yolo-py:version2\n",
    "```\n",
    "1. Then start the new version by \n",
    "```\n",
    "docker run --runtime=nvidia -dit airpollutionstudyindia/nvidia-yolo-py:version2\n",
    "docker exec -it cbb9ff138f81348 bash\n",
    "```\n",
    "1. the above not using GPU\n",
    "```\n",
    "nvidia-docker run --rm airpollutionstudyindia/nvidia-yolo-py:version2 nvidia-smi\n",
    "nvidia-docker run -dit airpollutionstudyindia/nvidia-yolo-py:version2 nvidia-smi\n",
    "```\n",
    "\n",
    "1. Run the code as follows\n",
    "```\n",
    "import pydarknet\n",
    "pydarknet.set_cuda_device(\"/gpu:0\")\n",
    "from pydarknet import Detector, Image\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "pngfiles=glob.glob('fsimgspa/images20180813_112806A/*/*.png')\n",
    "pngbatches=zip(*[iter(pngfiles)]*50)\n",
    "net = Detector(bytes(\"cfg/yolov3.cfg\", encoding=\"utf-8\"), bytes(\"weights/yolov3.weights\", encoding=\"utf-8\"), 0, bytes(\"cfg/coco.data\",encoding=\"utf-8\"))\n",
    "for idsb, batch in enumerate(pngbatches):\n",
    "    werf=[]\n",
    "    for ids, file_name in enumerate(batch):\n",
    "        img = cv2.imread(file_name)\n",
    "        img2 = Image(img)\n",
    "        results = net.detect(img2)\n",
    "        dbgh1=[]\n",
    "        print('complete',str(ids),'/',len(batch))\n",
    "        for cat, score, bounds in results:\n",
    "            resdict={}\n",
    "            resdict['cat']=cat\n",
    "            resdict['score']=score\n",
    "            resdict['bounds']=bounds\n",
    "            resdict['filename']=file_name\n",
    "            dbgh=pd.DataFrame.from_dict(resdict)\n",
    "            dbgh1.append(dbgh)\n",
    "        dbgh2=pd.concat(dbgh1)    \n",
    "        werf.append(dbgh2)\n",
    "    werf1=pd.concat(werf)\n",
    "    werf1.to_csv('output/batch_'+str(idsb)+'.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensure nividia-docker uses the GPU, using a benchmark described [here](https://marmelab.com/blog/2018/03/21/using-nvidia-gpu-within-docker-container.html)\n",
    "\n",
    "1. test every thing works, it shows no change in memeroy usage of GPU\n",
    "```\n",
    "docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi\n",
    "```\n",
    "1. made a bench mark script\n",
    "```\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "device_name = sys.argv[1]  # Choose device from cmd line. Options: gpu or cpu\n",
    "shape = (int(sys.argv[2]), int(sys.argv[2]))\n",
    "if device_name == \"gpu\":\n",
    "    device_name = \"/gpu:0\"\n",
    "else:\n",
    "    device_name = \"/cpu:0\"\n",
    "with tf.device(device_name):\n",
    "    random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1)\n",
    "    dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))\n",
    "    sum_operation = tf.reduce_sum(dot_operation)\n",
    "startTime = datetime.now()\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "        result = session.run(sum_operation)\n",
    "        print(result)\n",
    "# It can be hard to see the results on the terminal with lots of output -- add some newlines to improve readability.\n",
    "print(\"\\n\" * 5)\n",
    "print(\"Shape:\", shape, \"Device:\", device_name)\n",
    "print(\"Time taken:\", str(datetime.now() - startTime))\n",
    "```\n",
    "1. used the image mentioned in the page, for cpu, it gives, ('Time taken:', '0:01:27.997180')\n",
    "```\n",
    "docker run \\\n",
    "    --runtime=nvidia \\\n",
    "    --rm \\\n",
    "    -ti \\\n",
    "    -v \"${PWD}:/home/sunbird\" \\\n",
    "    tensorflow/tensorflow:latest-gpu \\\n",
    "    python /home/sunbird/benchmark.py cpu 2500\n",
    "```\n",
    "1. For GPU\n",
    "```\n",
    "docker run \\\n",
    "    --runtime=nvidia \\\n",
    "    --rm \\\n",
    "    -ti \\\n",
    "    -v \"${PWD}:/home/sunbird\" \\\n",
    "    tensorflow/tensorflow:latest-gpu \\\n",
    "    python /home/sunbird/benchmark.py gpu 2500\n",
    "```\n",
    "1. the baove ends in error\n",
    "```\n",
    "docker run \\\n",
    "    --runtime=nvidia \\\n",
    "    --rm \\\n",
    "    -ti \\\n",
    "    -v \"${PWD}:/home/sunbird\" \\\n",
    "    tensorflow/tensorflow:latest-gpu \\\n",
    "    python /home/sunbird/benchmark.py gpu 2500\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trail attempt 2\n",
    "\n",
    "1. use the pip3 installation, for that ```export CUDA_HOME=/usr/local/cuda-9.2``` has to be set \n",
    "```\n",
    "pip3 install yolo34py-gpu\n",
    "```\n",
    "1. the import of pydarknet has to be worked by running ```export DARKNET_HOME=/home/sunbird/darknet``` and ```export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$DARKNET_HOME```\n",
    "1. the code for a test\n",
    "```\n",
    "from pydarknet import Detector, Image\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "net = Detector(bytes(\"cfg/yolov3.cfg\", encoding=\"utf-8\"), bytes(\"yolov3.weights\", encoding=\"utf-8\"), 0, bytes(\"cfg/coco.data\",encoding=\"utf-8\"))\n",
    "file_name='/home/sunbird/images20180813_110045A/batch1/frame_219.0.png'\n",
    "img = cv2.imread(file_name)\n",
    "img2 = Image(img)\n",
    "results = net.detect(img2)\n",
    "```\n",
    "1. the above code works in no time, \n",
    "1. so running on the data\n",
    "```\n",
    "from pydarknet import Detector, Image\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "net = Detector(bytes(\"cfg/yolov3.cfg\", encoding=\"utf-8\"), bytes(\"yolov3.weights\", encoding=\"utf-8\"), 0, bytes(\"cfg/coco.data\",encoding=\"utf-8\"))\n",
    "def empty(seq):\n",
    "     try:\n",
    "        return all(map(empty, seq))\n",
    "     except TypeError:\n",
    "        return False\n",
    "pngfiles=glob.glob('/home/sunbird/images20180813_110045A/*/*.png')\n",
    "#pngfiles=glob.glob('/home/sunbird/images20180813_114233A/*/*.png') \n",
    "pngfiles=glob.glob('/home/sunbird/images/*/*/*.png')\n",
    "pngbatches=zip(*[iter(pngfiles)]*50)\n",
    "for idsb, batch in enumerate(list(pngbatches)):\n",
    "    werf=[]\n",
    "    for ids, file_name in enumerate(batch):\n",
    "        print(file_name)\n",
    "        img = cv2.imread(file_name)\n",
    "        img2 = Image(img)\n",
    "        results = net.detect(img2)\n",
    "        dbgh1=[]\n",
    "        print('complete',str(ids),'/',len(batch))\n",
    "        print(results)\n",
    "        if not results:\n",
    "            resdict={}\n",
    "            resdict['cat']='nocat'\n",
    "            resdict['score']=0\n",
    "            resdict['bounds']=(0,0,0,0)\n",
    "            resdict['filename']=file_name\n",
    "            dbgh=pd.DataFrame.from_records([resdict])\n",
    "            dbgh1.append(dbgh)\n",
    "        else:\n",
    "            for cat, score, bounds in results:\n",
    "                resdict={}\n",
    "                resdict['cat']=cat\n",
    "                resdict['score']=score\n",
    "                resdict['bounds']=bounds\n",
    "                resdict['filename']=file_name\n",
    "                dbgh=pd.DataFrame.from_records([resdict])\n",
    "                dbgh1.append(dbgh)\n",
    "        dbgh2=pd.concat(dbgh1)    \n",
    "        werf.append(dbgh2)\n",
    "    werf1=pd.concat(werf)\n",
    "    werf1.to_csv('output_images/batch_'+str(idsb)+'.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat, score, bounds in results:\n",
    "    resdict={}\n",
    "    resdict['cat']=cat\n",
    "    resdict['score']=score\n",
    "    resdict['bounds']=bounds\n",
    "    resdict['filename']=file_name\n",
    "    dbgh=pd.DataFrame.from_dict(resdict)\n",
    "    dbgh1.append(dbgh)\n",
    "    \n",
    "df = pd.DataFrame.from_records([{ 'A':a,'B':b }], index='A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idsb, batch in enumerate(pngbatches):\n",
    "    werf=[]\n",
    "    for ids, file_name in enumerate(batch):\n",
    "        img = cv2.imread(file_name)\n",
    "        img2 = Image(img)\n",
    "        results = net.detect(img2)\n",
    "        dbgh1=[]\n",
    "        print('complete',str(ids),'/',len(batch))\n",
    "        for cat, score, bounds in results:\n",
    "            resdict={}\n",
    "            resdict['cat']=cat\n",
    "            resdict['score']=score\n",
    "            resdict['bounds']=bounds\n",
    "            resdict['filename']=file_name\n",
    "            dbgh=pd.DataFrame.from_dict(resdict)\n",
    "            dbgh1.append(dbgh)\n",
    "        dbgh2=pd.concat(dbgh1)    \n",
    "        werf.append(dbgh2)\n",
    "    werf1=pd.concat(werf)\n",
    "    werf1.to_csv('output/batch_'+str(idsb)+'.csv')\n",
    "    \n",
    "    \n",
    "for cat, score, bounds in results:\n",
    "    resdict={}\n",
    "    resdict['cat']=cat\n",
    "    resdict['score']=score\n",
    "    resdict['bounds']=bounds\n",
    "    resdict['filename']=file_name\n",
    "    print (resdict)\n",
    "    \n",
    "    \n",
    "resdict={}\n",
    "resdict['cat']='nocat'\n",
    "resdict['score']='noscore'\n",
    "resdict['bounds']='nobounds'\n",
    "resdict['filename']=file_name\n",
    "dbgh=pd.DataFrame.from_dict(resdict)\n",
    "dbgh1.append(dbgh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating videos from the csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. run container with cv2\n",
    "```\n",
    "sudo mount -o discard,defaults /dev/sdd /home/sunbird\n",
    "sudo chmod a+w /home/sunbird\n",
    "sudo systemctl restart docker\n",
    "docker run -dit airpollutionstudyindia/yolo-python:version1\n",
    "docker exec -it 96977822d57305 bash\n",
    "sudo docker cp /home/sunbird/output_images.zip 96977822d5730:/home/ \n",
    "pip3 install moviepy pandas requests\n",
    "gcloud compute scp /home/sunbird/Documents/UEmissInfo/lab/emission-industry/fieldwork/odk/patna/dashcam/output_images.zip instance-4:/home/sunbird/\n",
    "output_images.zip    \n",
    "zip -r ann_images.zip ann_images\n",
    "sudo rm ann_images.zip \n",
    "sudo docker cp 03924f00e4470ce:/home/ann_images.zip /home/sunbird/yolo_img/\n",
    "gcloud compute scp instance-4:/home/sunbird/yolo_img/ann_images.zip /home/sunbird/Documents/UEmissInfo/lab/emission-industry/fieldwork/odk/patna/dashcam/\n",
    "sudo docker cp 96977822d573:/home/fusspatc.zip /home/sunbird/yolo_img/\n",
    "gcloud compute scp instance-4:/home/sunbird/yolo_img/fusspatc.zip /home/sunbird/Documents/UEmissInfo/lab/emission-industry/fieldwork/odk/patna/dashcam/\n",
    "```\n",
    "1. the run code is \n",
    "```\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import glob\n",
    "import ntpath\n",
    "import os\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "csvfiles=glob.glob('/home/output_images/*.csv')\n",
    "db1=[]\n",
    "for csvf in csvfiles:\n",
    "    db=pd.read_csv(csvf)\n",
    "    db1.append(db)\n",
    "db2=pd.concat(db1)\n",
    "db2['boundtup'] = [literal_eval(x) for x in db2['bounds']]\n",
    "db2['newflname']=db2['filename'].str[20:]\n",
    "db2['cat1']=db2['cat'].str[2:-1]\n",
    "db2['movname']=db2['filename'].str[27:-23]\n",
    "db3=db2.reset_index()\n",
    "db4=db3.drop_duplicates('filename')\n",
    "db4['img_no']=np.arange(0,len(db4.index))\n",
    "db5=db4[['img_no']]\n",
    "db3=db3.join(db5)\n",
    "db3['img_no']=db3['img_no'].fillna(method='ffill')\n",
    "db4=db3.drop_duplicates('img_no')\n",
    "filenumberlist=db4['img_no'].tolist()\n",
    "folders=['images20180813_110045A','images20180813_112806A','images20180813_114233A']\n",
    "foldb2=[]\n",
    "for fold in folders:\n",
    "    foldb1=db3[db3['filename'].str.contains(fold)]\n",
    "    foldb1['folder']=fold\n",
    "    foldb2.append(foldb1)\n",
    "foldb3=pd.concat(foldb2)\n",
    "#foldb3.info()\n",
    "for ids, filenumbers in enumerate(filenumberlist):\n",
    "    print(filenumbers)\n",
    "    mdb5=foldb3[foldb3['img_no']==filenumbers]\n",
    "    img = cv2.imread('/home/images'+mdb5['newflname'].iloc[0])\n",
    "    shpfiles1=(os.path.splitext(mdb5['filename'].iloc[0]))\n",
    "    head, tail = ntpath.split(shpfiles1[0])\n",
    "    foldername=mdb5['folder'].iloc[0]\n",
    "    for idx, row in mdb5.iterrows():\n",
    "        x, y, w, h = row['boundtup']\n",
    "        cv2.rectangle(img, (int(x - w / 2), int(y - h / 2)), (int(x + w / 2), int(y + h / 2)), (255, 0, 0), thickness=2)\n",
    "        cv2.putText(img,row['cat1'],(int(x),int(y)),cv2.FONT_HERSHEY_DUPLEX,0.8,(0,0,255), thickness=1)\n",
    "    cv2.imwrite('/home/ann_images/'+foldername+'_'+tail+'.png',img)\n",
    "    print('complete',str(ids),'/',len(filenumberlist))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2\n",
    "import pandas as pd\n",
    "import glob\n",
    "import ntpath\n",
    "import os\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "csvfiles=glob.glob('/home/sunbird/Documents/UEmissInfo/lab/emission-industry/fieldwork/odk/patna/dashcam/output_images/*.csv')\n",
    "\n",
    "db1=[]\n",
    "for csvf in csvfiles:\n",
    "    db=pd.read_csv(csvf)\n",
    "    db1.append(db)\n",
    "\n",
    "db2=pd.concat(db1)\n",
    "db2['bounds'] = [literal_eval(x) for x in db2['bounds']]\n",
    "#db2['newflname']=db2['filename'].str[20:]\n",
    "db2['cat']=db2['cat'].str[2:-1]\n",
    "#db2['movname']=db2['filename'].str[27:-23]\n",
    "db3=db2.reset_index()\n",
    "db4=db3[[u'bounds', u'cat', u'filename', u'score']]\n",
    "db4.to_csv('/home/sunbird/Dropbox/4Nishadh/fieldstudy/patna/dash_cam/raw_data.csv')\n",
    "### get the folder name\n",
    "# folders=['images20180813_110045A','images20180813_112806A','images20180813_114233A']\n",
    "# foldb2=[]\n",
    "# for fold in folders:\n",
    "#     foldb1=db2[db2['filename'].str.contains(fold)]\n",
    "#     foldb1['folder']=fold\n",
    "#     foldb2.append(foldb1)\n",
    "# foldb3=pd.concat(foldb2)\n",
    "#foldb3.info()\n",
    "\n",
    "\n",
    "\n",
    "# db3=db2.reset_index()\n",
    "# db3['originindex']=db3.index\n",
    "# db4=db3.drop_duplicates('filename')\n",
    "# db4['img_no']=np.arange(0,len(db4.index))\n",
    "# db5=db4[['originindex','img_no']]\n",
    "\n",
    "# db2\n",
    "# # imdb3=pd.merge(db3,db5, on='originindex',how='left')\n",
    "# # imdb3['img_no']=imdb3['img_no'].fillna(method='ffill')\n",
    "\n",
    "# # imdb4=imdb3.drop_duplicates('img_no')\n",
    "# # filenumberlist=imdb4['img_no'].tolist()\n",
    "\n",
    "# # for ids, filenumbers in enumerate(filenumberlist[0:3]):\n",
    "# #     mdb5=imdb3[imdb3['img_no']==filenumbers]\n",
    "# #     print(mdb5['filename'].iloc[0],'/home/images'+mdb5['newflname'].iloc[0])\n",
    "# #     #img = cv2.imread('/home/images'+db5['newflname'].iloc[0])\n",
    "# #     shpfiles1=(os.path.splitext(mdb5['filename'].iloc[0]))\n",
    "# #     head, tail = ntpath.split(shpfiles1[0])\n",
    "\n",
    "#db4=foldb3.drop_duplicates('folder')\n",
    "#db4\n",
    "\n",
    "# dfg=foldb3[foldb3['folder']==folders[0]]\n",
    "# dfg\n",
    "#db3=db2.drop_duplicates('cat1')\n",
    "#db3=db2[db2['cat1'].str.contains('bicycle|truck|car|bus|motorbike|cow')]\n",
    "#db3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "max_height = 360\n",
    "max_width = 360\n",
    "\n",
    "imgfiles=glob.glob('/home/ann_images/*.png')\n",
    "\n",
    "for imgs in imgfiles:\n",
    "    shpfiles1=os.path.splitext(imgs)\n",
    "    head, tail = ntpath.split(shpfiles1[0])\n",
    "    print(tail)\n",
    "    img = cv2.imread(imgs)\n",
    "    height, width = img.shape[:2]\n",
    "    # only shrink if img is bigger than required\n",
    "    if max_height < height or max_width < width:\n",
    "        # get scaling factor\n",
    "        scaling_factor = max_height / float(height)\n",
    "        if max_width/float(width) < scaling_factor:\n",
    "            scaling_factor = max_width / float(width)\n",
    "        # resize image\n",
    "        img = cv2.resize(img, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "        cv2.imwrite('/home/resize_annimages/'+tail+'.png',img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import ntpath\n",
    "import pandas as pd\n",
    "import moviepy.editor as mpy\n",
    "import os\n",
    "\n",
    "\n",
    "imgfiles=glob.glob('/home/resize_annimages/*.png')\n",
    "\n",
    "folbuc1=[]\n",
    "fnbuc1=[]\n",
    "fnobuc1=[]\n",
    "for imgs in imgfiles:\n",
    "    shpfiles1=os.path.splitext(imgs)\n",
    "    head, tail = ntpath.split(shpfiles1[0])\n",
    "    folder=tail.split('_')\n",
    "    folder1=folder[0]+'_'+folder[1]\n",
    "    filno=folder[3]\n",
    "    filename=folder[2]+'_'+folder[3]\n",
    "    folbuc1.append(folder1)\n",
    "    fnbuc1.append(filename)\n",
    "    fnobuc1.append(filno)\n",
    "\n",
    "db=pd.DataFrame()\n",
    "db['folder']=folbuc1\n",
    "db['filename']=fnbuc1\n",
    "db['fno']=fnobuc1\n",
    "db['fullname']=imgfiles\n",
    "\n",
    "db['fno'] = pd.to_numeric(db['fno'])\n",
    "\n",
    "folders=['images20180813_110045A','images20180813_112806A','images20180813_114233A']\n",
    "\n",
    "\n",
    "for fol in folders:\n",
    "    db1=db[db['folder']==fol]\n",
    "    db2=db1.sort_values('fno')\n",
    "    pngfiles=db2['fullname'].unique().tolist()\n",
    "    output_base_name='/home/fusspatc/fusspatc_'+str(fol)+'.mp4'\n",
    "    aa=[1]*len(pngfiles)\n",
    "    clip = mpy.ImageSequenceClip(pngfiles, durations=aa, load_images=True)\n",
    "    clip.write_videofile(output_base_name,audio=False,fps=24 )\n",
    "    print(\"done with\",str(fol))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1=[]\n",
    "for csvf in csvfiles:\n",
    "    db=pd.read_csv(csvf)\n",
    "    db1.append(db)\n",
    "\n",
    "db2=pd.concat(db1)\n",
    "db2['boundtup'] = [literal_eval(x) for x in db2['bounds']]\n",
    "db2['newflname']=db['filename'].str[20:]\n",
    "db3['movname']=db3['filename'].str[20:]\n",
    "db3=db2.reset_index()\n",
    "db3['originindex']=db3.index()\n",
    "db4=db3.drop_duplicates('filename')\n",
    "db4['img_no']=np.arange(0,len(db4.index))\n",
    "db5=db4[['img_no']]\n",
    "\n",
    "\n",
    "db3=\n",
    "db3['img_no']=db3['img_no'].fillna(method='ffill')\n",
    "\n",
    "db3.to_csv('/home/sunbird/Documents/UEmissInfo/lab/emission-industry/fieldwork/odk/patna/dashcam/output_images_test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=db3['filename'].unique().tolist()\n",
    "len(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a video with high resolution images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "db=pd.read_csv('/home/raw_data.csv')\n",
    "\n",
    "db1=db[db['filename'].str.contains('images20180813_114233A/batch5')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import glob\n",
    "import ntpath\n",
    "import os\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "\n",
    "db=pd.read_csv('/home/raw_data.csv')\n",
    "\n",
    "db2=db[db['filename'].str.contains('images20180813_114233A/batch5')]\n",
    "\n",
    "db2['boundtup'] = [literal_eval(x) for x in db2['bounds']]\n",
    "db2['newflname']=db2['filename'].str[20:]\n",
    "#db2['cat1']=db2['cat'].str[2:-1]\n",
    "db2['movname']=db2['filename'].str[27:-23]\n",
    "db3=db2.reset_index()\n",
    "db4=db3.drop_duplicates('filename')\n",
    "db4['img_no']=np.arange(0,len(db4.index))\n",
    "db5=db4[['img_no']]\n",
    "db3=db3.join(db5)\n",
    "db3['img_no']=db3['img_no'].fillna(method='ffill')\n",
    "db4=db3.drop_duplicates('img_no')\n",
    "filenumberlist=db4['img_no'].tolist()\n",
    "\n",
    "# folders=['images20180813_110045A','images20180813_112806A','images20180813_114233A']\n",
    "# foldb2=[]\n",
    "# for fold in folders:\n",
    "#     foldb1=db3[db3['filename'].str.contains(fold)]\n",
    "#     foldb1['folder']=fold\n",
    "#     foldb2.append(foldb1)\n",
    "    \n",
    "# foldb3=pd.concat(foldb2)\n",
    "# #foldb3.info()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for ids, filenumbers in enumerate(filenumberlist):\n",
    "    print(filenumbers)\n",
    "    mdb5=db3[db3['img_no']==filenumbers]\n",
    "    img = cv2.imread('/media/sunbird/10a367c4-6c6a-42f7-b481-58ac6ef2d83d/home/hoopoe/Documents/UEmissInfo/lab/emission-industry/fieldwork/odk/patna/dashcam/aug13/morning/'+mdb5['newflname'].iloc[0])\n",
    "    shpfiles1=(os.path.splitext(mdb5['filename'].iloc[0]))\n",
    "    head, tail = ntpath.split(shpfiles1[0])\n",
    "    for idx, row in mdb5.iterrows():\n",
    "        x, y, w, h = row['boundtup']\n",
    "        cv2.rectangle(img, (int(x - w / 2), int(y - h / 2)), (int(x + w / 2), int(y + h / 2)), (255, 0, 0), thickness=2)\n",
    "        cv2.putText(img,row['cat'],(int(x),int(y)),cv2.FONT_HERSHEY_DUPLEX,0.6,(0,0,255), thickness=1)\n",
    "    cv2.imwrite('/home/sunbird/Documents/UEmissInfo/lab/emission-industry/fieldwork/patna/HD_video/images/'+tail+'.png',img)\n",
    "    print('complete',str(ids),'/',len(filenumberlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T18:28:26.950365Z",
     "start_time": "2022-05-08T18:28:25.260982Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import ntpath\n",
    "import pandas as pd\n",
    "import moviepy.editor as mpy\n",
    "import os\n",
    "\n",
    "\n",
    "imgfiles=glob.glob('/home/images/*.png')\n",
    "\n",
    "folbuc1=[]\n",
    "fnbuc1=[]\n",
    "fnobuc1=[]\n",
    "for imgs in imgfiles:\n",
    "    shpfiles1=os.path.splitext(imgs)\n",
    "    head, tail = ntpath.split(shpfiles1[0])\n",
    "    folder=tail.split('_')\n",
    "    folder1=folder[0]+'_'+folder[1]\n",
    "    filno=folder[1]\n",
    "    #filename=folder[2]+'_'+folder[3]\n",
    "    folbuc1.append(folder1)\n",
    "    #fnbuc1.append(filename)\n",
    "    fnobuc1.append(filno)\n",
    "\n",
    "db=pd.DataFrame()\n",
    "db['folder']=folbuc1\n",
    "#db['filename']=fnbuc1\n",
    "db['fno']=fnobuc1\n",
    "db['fullname']=imgfiles\n",
    "\n",
    "db['fno'] = pd.to_numeric(db['fno'])\n",
    "\n",
    "db1=db.sort_values('fno')\n",
    "db2=db1[80:95]\n",
    "\n",
    "pngfiles=db2['fullname'].unique().tolist()\n",
    "output_base_name='/home/fusspatc.mp4'\n",
    "aa=[1]*len(pngfiles)\n",
    "clip = mpy.ImageSequenceClip(pngfiles, durations=aa, load_images=True)\n",
    "clip.write_videofile(output_base_name,audio=False,fps=24 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "31px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
